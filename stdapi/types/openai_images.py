"""Local OpenAI-compatible image generation types."""

from typing import Literal, Self

from pydantic import Field, model_validator

from stdapi.config import SETTINGS
from stdapi.monitoring import log_error_details
from stdapi.types import BaseModelRequestWithExtra, BaseModelResponse
from stdapi.types.openai import Auto

#: Supported image output formats
ImageOutputFormats = Literal["png", "jpeg", "webp"]

#: Supported quality output level
ImageOutputQuality = Literal["low", "medium", "high"]
ImageOutputQualityAuto = Auto | ImageOutputQuality

#: Supported image background
ImageBackground = Literal["transparent", "opaque"]
ImageBackgroundAuto = Auto | ImageBackground


# Ref: openai.types.image.Image
class Image(BaseModelResponse):
    """Generated image descriptor compatible with OpenAI."""

    b64_json: str | None = Field(
        default=None,
        description=(
            "The base64-encoded JSON of the generated image., "
            "Only present if `response_format` is set to `b64_json`."
        ),
    )
    revised_prompt: str | None = Field(
        default=None,
        description="The revised prompt that was used to generate the image.",
    )
    url: str | None = Field(
        default=None,
        description=(
            "The URL of the generated image if `response_format` is set to `url`."
        ),
    )


# Ref: openai.types.images_response.UsageInputTokensDetails
class UsageInputTokensDetails(BaseModelResponse):
    """Detailed input token usage for image generation."""

    image_tokens: int = Field(
        default=0, ge=0, description="The number of image tokens in the input prompt."
    )
    text_tokens: int = Field(
        default=0, ge=0, description="The number of text tokens in the input prompt."
    )


# Ref: openai.types.images_response.Usage
class Usage(BaseModelResponse):
    """Image generation token usage information."""

    input_tokens: int = Field(
        default=0,
        ge=0,
        description="The number of tokens (images and text) in the input prompt.",
    )
    input_tokens_details: UsageInputTokensDetails = Field(
        description="The input tokens detailed information for the image generation."
    )
    output_tokens: int = Field(
        default=0,
        ge=0,
        description="The number of output tokens generated by the model.",
    )
    total_tokens: int = Field(
        default=0,
        ge=0,
        description=(
            "The total number of tokens (images and text) used for the image generation."
        ),
    )


# Ref: openai.types.images_response.ImagesResponse
class ImagesResponse(BaseModelResponse):
    """OpenAI-compatible non-streaming image generation response."""

    created: int = Field(
        ge=0,
        description="The Unix timestamp (in seconds) of when the image was created.",
    )
    background: ImageBackground | None = Field(
        default=None,
        description=(
            "The background parameter used for the image generation. Either `transparent` or `opaque`."
        ),
    )
    data: list[Image] | None = Field(
        default=None, description="The list of generated images."
    )
    output_format: ImageOutputFormats | None = Field(
        default=None,
        description=(
            "The output format of the image generation. Either `png`, `webp`, or `jpeg`."
        ),
    )
    quality: ImageOutputQuality | None = Field(
        default=None, description="The quality of the generated image."
    )
    size: str | None = Field(
        default=None, description="The size of the generated image."
    )
    usage: Usage | None = Field(
        default=None,
        description="The token usage information for the image generation.",
    )


# Ref: openai.types.image_gen_completed_event.ImageGenCompletedEvent
class ImageGenCompletedEvent(BaseModelResponse):
    """Streaming event emitted when image generation completes."""

    b64_json: str = Field(
        description="Base64-encoded image data, suitable for rendering as an image."
    )
    background: ImageBackgroundAuto = Field(
        description="The background setting for the generated image."
    )
    created_at: int = Field(
        ge=0, description="The Unix timestamp when the event was created."
    )
    output_format: ImageOutputFormats = Field(
        description="The output format for the generated image."
    )
    quality: ImageOutputQualityAuto = Field(
        description="The quality setting for the generated image."
    )
    size: str | None = Field(
        default=None, description="The size of the generated image."
    )
    type: Literal["image_generation.completed"] = Field(
        description="The type of the event. Always `image_generation.completed`."
    )
    usage: Usage = Field(
        description="The token usage information for the image generation."
    )


# Ref: openai.types.image_gen_partial_image_event.ImageGenPartialImageEvent
class ImageGenPartialImageEvent(BaseModelResponse):
    """Streaming event emitted for partial images during generation."""

    b64_json: str = Field(
        description="Base64-encoded partial image data, suitable for rendering as an image."
    )
    background: ImageBackgroundAuto = Field(
        description="The background setting for the requested image."
    )
    created_at: int = Field(
        ge=0, description="The Unix timestamp when the event was created."
    )
    output_format: ImageOutputFormats = Field(
        description="The output format for the requested image."
    )
    partial_image_index: int = Field(
        ge=0, description="0-based index for the partial image (streaming)."
    )
    quality: ImageOutputQualityAuto = Field(
        description="The quality setting for the requested image."
    )
    size: str | None = Field(
        default=None, description="The size of the generated image."
    )
    type: Literal["image_generation.partial_image"] = Field(
        description="The type of the event. Always `image_generation.partial_image`."
    )


# Ref: openai.types.image_generate_params.ImageGenerateParams
class ImageGenerateParams(BaseModelRequestWithExtra):
    """Request body for generating images."""

    prompt: str = Field(
        ..., description="A text description of the desired image(s).", min_length=1
    )
    background: ImageBackgroundAuto = Field(
        description="Allows to set transparency for the background of the generated image(s).\n"
        "If `transparent`, the output format needs to support transparency, "
        "so it should be set to either `png` (default value) or `webp`."
        "\ntransparent is UNSUPPORTED on this implementation.",
        default="auto",
    )
    model: str = Field(
        description="The model to use for image generation.",
        min_length=1,
        max_length=255,
    )
    moderation: Literal["low", "auto"] = Field(
        description="Control the content-moderation level for generated images.\n"
        "Must be either `low` for less restrictive filtering or `auto` (default value)."
        "\nlow is UNSUPPORTED on this implementation.",
        default="auto",
    )
    n: int = Field(
        default=1, description="The number of images to generate.", ge=1, le=10
    )
    output_compression: int = Field(
        description="The compression level (0-100%) for the generated images.",
        default=100,
        ge=1,
        le=100,
    )
    output_format: ImageOutputFormats | None = Field(
        default=None,
        description="The format in which the generated images are returned. "
        "Must be one of `png`, `jpeg`, or `webp`.",
    )
    partial_images: int | None = Field(
        description="The number of partial images to generate.\n"
        "This parameter is used for streaming responses that return partial images. "
        "Value must be between 0 and 3. "
        "When set to 0, the response will be a single image sent in one streaming event.\n"
        "Note that the final image may be sent before the full number of partial images "
        "are generated if the full image is generated more quickly.\n"
        "Partial images are only sent if the model supports it.",
        ge=0,
        le=3,
        default=None,
    )
    quality: str = Field(  # Support different values than OpenAI
        default="auto",
        description="The quality of the image that will be generated.\n"
        "`auto` (default value) will automatically select the best quality for the given model.\n"
        "Supported values depend on the model.",
        min_length=1,
        max_length=255,
    )
    response_format: Literal["url", "b64_json"] = Field(
        default="url",
        description="The format in which the generated images are returned. "
        "Must be one of url or b64_json. "
        "URLs are only valid for 60 minutes after the image has been generated.\n"
        "This parameter isn't supported with streaming which will always "
        "return base64-encoded images.",
    )
    size: str = Field(  # Support different values than OpenAI
        default="1024x1024",
        pattern=r"^(\d+)x(\d+)$",
        description="The size of the generated images."
        "\nSupported values depend on the model. "
        "With some models, output size may be different.",
    )
    style: str | None = Field(  # Support different values than OpenAI
        default=None,
        description="The style of the generated images.\n"
        "Supported values depend on the model.",
        min_length=1,
        max_length=255,
    )
    user: str | None = Field(
        default=None,
        description="A unique identifier representing your end-user, which can help to monitor and detect abuse.",
        min_length=1,
        max_length=255,
    )
    stream: bool = Field(
        default=False, description="Generate the image in streaming mode."
    )

    @model_validator(mode="after")
    def _unsupported(self) -> Self:
        """Validate unsupported or incompatible options.

        Raises:
            ValueError: When an unsupported option is requested or options are incompatible.
        """
        if self.partial_images is not None and not self.stream:
            msg = "partial_images requires streaming mode."
            raise ValueError(msg)
        if self.background == "transparent":
            msg = "Background transparency is not supported on this backend."
            raise ValueError(msg)
        if self.moderation != "auto":
            msg = "The 'moderation' parameter is not supported on this backend."
            raise ValueError(msg)
        if self.response_format == "url" and not SETTINGS.aws_s3_bucket:
            log_error_details(
                "No S3 bucket configured for presigned URLs. "
                "AWS_S3_BUCKET environment variable is not set."
            )
            msg = (
                "The url response format is not enabled on this server. "
                "Please contact the administrator to enabled it."
            )
            raise ValueError(msg)
        return self
